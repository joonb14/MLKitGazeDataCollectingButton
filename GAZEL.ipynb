{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target=\"/special/jbpark/TabS6LData/Joonbeom/train_dataset/\"\n",
    "\n",
    "left_eye_right_top = np.load(target+\"left_eye_right_top.npy\")\n",
    "left_eye_left_bottom = np.load(target+\"left_eye_left_bottom.npy\")\n",
    "right_eye_right_top = np.load(target+\"right_eye_right_top.npy\")\n",
    "right_eye_left_bottom = np.load(target+\"right_eye_left_bottom.npy\")\n",
    "\n",
    "\n",
    "left_eye_right_top[:,1] = left_eye_right_top[:,1] - left_eye_left_bottom[:,1]\n",
    "left_eye_right_top[:,0] = left_eye_left_bottom[:,0] - left_eye_right_top[:,0]\n",
    "\n",
    "right_eye_right_top[:,1] = right_eye_right_top[:,1] - right_eye_left_bottom[:,1]\n",
    "right_eye_right_top[:,0] = right_eye_left_bottom[:,0] - right_eye_right_top[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[236.05314, 236.0531 ],\n",
       "       [230.8075 , 230.8074 ],\n",
       "       [251.79   , 251.7901 ],\n",
       "       ...,\n",
       "       [220.31616, 220.3162 ],\n",
       "       [230.8075 , 230.8074 ],\n",
       "       [230.80749, 230.8074 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_eye_right_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[230.80755, 230.80744],\n",
       "       [230.80745, 230.8074 ],\n",
       "       [246.54435, 246.5444 ],\n",
       "       ...,\n",
       "       [230.80752, 230.8075 ],\n",
       "       [225.56189, 225.562  ],\n",
       "       [225.56195, 225.5619 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_eye_right_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "left_eye (InputLayer)           [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_eye (InputLayer)          [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         left_eye[0][0]                   \n",
      "                                                                 right_eye[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 32)   0           conv2d[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_1[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 64)   36928       max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_2[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 4096)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "left_eye_right_top (InputLayer) [(None, 1, 1, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "right_eye_right_top (InputLayer [(None, 1, 1, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "euler (InputLayer)              [(None, 1, 1, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "facepos (InputLayer)            [(None, 1, 1, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2)            0           left_eye_right_top[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 2)            0           right_eye_right_top[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           524352      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 3)            0           euler[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2)            0           facepos[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4)            0           flatten_4[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1040        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 9)            0           flatten_3[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           160         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32)           0           dropout[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           528         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pred (Dense)                    (None, 2)            34          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2)            0           pred[0][0]                       \n",
      "                                                                 flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 581,858\n",
      "Trainable params: 581,858\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 4091 samples, validate on 455 samples\n",
      "Epoch 1/1000\n",
      "4091/4091 [==============================] - 2s 585us/sample - loss: 824.0444 - val_loss: 793.5513\n",
      "Epoch 2/1000\n",
      "4091/4091 [==============================] - 1s 352us/sample - loss: 764.3626 - val_loss: 769.2853\n",
      "Epoch 3/1000\n",
      "4091/4091 [==============================] - 1s 342us/sample - loss: 756.4690 - val_loss: 762.8087\n",
      "Epoch 4/1000\n",
      "4091/4091 [==============================] - 1s 346us/sample - loss: 754.5714 - val_loss: 762.6443\n",
      "Epoch 5/1000\n",
      "4091/4091 [==============================] - 1s 344us/sample - loss: 754.9941 - val_loss: 760.4310\n",
      "Epoch 6/1000\n",
      "4091/4091 [==============================] - 1s 341us/sample - loss: 743.8414 - val_loss: 728.8050\n",
      "Epoch 7/1000\n",
      "4091/4091 [==============================] - 1s 346us/sample - loss: 557.9934 - val_loss: 479.5256\n",
      "Epoch 8/1000\n",
      "4091/4091 [==============================] - 1s 339us/sample - loss: 456.8109 - val_loss: 459.0128\n",
      "Epoch 9/1000\n",
      "4091/4091 [==============================] - 1s 346us/sample - loss: 438.9573 - val_loss: 431.1682\n",
      "Epoch 10/1000\n",
      "4091/4091 [==============================] - 1s 348us/sample - loss: 429.9633 - val_loss: 421.8247\n",
      "Epoch 11/1000\n",
      "4091/4091 [==============================] - 1s 340us/sample - loss: 425.6255 - val_loss: 419.3299\n",
      "Epoch 12/1000\n",
      "4091/4091 [==============================] - 1s 338us/sample - loss: 420.9972 - val_loss: 413.9388\n",
      "Epoch 13/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 417.5683 - val_loss: 419.8232\n",
      "Epoch 14/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 416.9752 - val_loss: 417.3016\n",
      "Epoch 15/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 414.9077 - val_loss: 422.6293\n",
      "Epoch 16/1000\n",
      "4091/4091 [==============================] - 1s 343us/sample - loss: 411.7184 - val_loss: 409.3994\n",
      "Epoch 17/1000\n",
      "4091/4091 [==============================] - 1s 341us/sample - loss: 408.2056 - val_loss: 405.3062\n",
      "Epoch 18/1000\n",
      "4091/4091 [==============================] - 1s 346us/sample - loss: 403.9137 - val_loss: 399.3568\n",
      "Epoch 19/1000\n",
      "4091/4091 [==============================] - 1s 343us/sample - loss: 384.3712 - val_loss: 370.1325\n",
      "Epoch 20/1000\n",
      "4091/4091 [==============================] - 1s 339us/sample - loss: 274.9835 - val_loss: 232.5523\n",
      "Epoch 21/1000\n",
      "4091/4091 [==============================] - 1s 341us/sample - loss: 182.7123 - val_loss: 156.4279\n",
      "Epoch 22/1000\n",
      "4091/4091 [==============================] - 1s 347us/sample - loss: 161.2247 - val_loss: 145.9255\n",
      "Epoch 23/1000\n",
      "4091/4091 [==============================] - 1s 346us/sample - loss: 145.6078 - val_loss: 132.1811\n",
      "Epoch 24/1000\n",
      "4091/4091 [==============================] - 1s 338us/sample - loss: 137.7684 - val_loss: 129.4265\n",
      "Epoch 25/1000\n",
      "4091/4091 [==============================] - 1s 317us/sample - loss: 120.2104 - val_loss: 130.5925\n",
      "Epoch 26/1000\n",
      "4091/4091 [==============================] - 1s 345us/sample - loss: 113.3554 - val_loss: 109.6069\n",
      "Epoch 27/1000\n",
      "4091/4091 [==============================] - 1s 348us/sample - loss: 108.9007 - val_loss: 106.6572\n",
      "Epoch 28/1000\n",
      "4091/4091 [==============================] - 1s 363us/sample - loss: 106.8717 - val_loss: 100.8748\n",
      "Epoch 29/1000\n",
      "4091/4091 [==============================] - 1s 351us/sample - loss: 105.1654 - val_loss: 99.7056\n",
      "Epoch 30/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 102.4704 - val_loss: 126.1001\n",
      "Epoch 31/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 103.7535 - val_loss: 102.8274\n",
      "Epoch 32/1000\n",
      "4091/4091 [==============================] - 1s 343us/sample - loss: 102.6215 - val_loss: 97.9876\n",
      "Epoch 33/1000\n",
      "4091/4091 [==============================] - 1s 346us/sample - loss: 97.6904 - val_loss: 90.9655\n",
      "Epoch 34/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 94.7903 - val_loss: 93.2222\n",
      "Epoch 35/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 94.0385 - val_loss: 111.1153\n",
      "Epoch 36/1000\n",
      "4091/4091 [==============================] - 1s 343us/sample - loss: 90.8151 - val_loss: 88.7637\n",
      "Epoch 37/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 87.2887 - val_loss: 94.5808\n",
      "Epoch 38/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 91.2785 - val_loss: 101.4726\n",
      "Epoch 39/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 88.7716 - val_loss: 95.9969\n",
      "Epoch 40/1000\n",
      "4091/4091 [==============================] - 1s 341us/sample - loss: 86.2572 - val_loss: 86.8191\n",
      "Epoch 41/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 85.7388 - val_loss: 100.9796\n",
      "Epoch 42/1000\n",
      "4091/4091 [==============================] - 1s 345us/sample - loss: 86.0959 - val_loss: 82.4513\n",
      "Epoch 43/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 83.9922 - val_loss: 85.7951\n",
      "Epoch 44/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 84.1681 - val_loss: 88.6745\n",
      "Epoch 45/1000\n",
      "4091/4091 [==============================] - 1s 317us/sample - loss: 82.3831 - val_loss: 83.1952\n",
      "Epoch 46/1000\n",
      "4091/4091 [==============================] - 1s 339us/sample - loss: 80.9208 - val_loss: 80.1409\n",
      "Epoch 47/1000\n",
      "4091/4091 [==============================] - 1s 342us/sample - loss: 84.3390 - val_loss: 80.0037\n",
      "Epoch 48/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 82.2612 - val_loss: 111.5485\n",
      "Epoch 49/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 78.8136 - val_loss: 80.4179\n",
      "Epoch 50/1000\n",
      "4091/4091 [==============================] - 1s 341us/sample - loss: 81.2160 - val_loss: 77.0679\n",
      "Epoch 51/1000\n",
      "4091/4091 [==============================] - 1s 317us/sample - loss: 76.8825 - val_loss: 103.3429\n",
      "Epoch 52/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 80.1173 - val_loss: 98.9859\n",
      "Epoch 53/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 78.8852 - val_loss: 102.1743\n",
      "Epoch 54/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 74.0845 - val_loss: 82.0755\n",
      "Epoch 55/1000\n",
      "4091/4091 [==============================] - 1s 346us/sample - loss: 73.8417 - val_loss: 74.6336\n",
      "Epoch 56/1000\n",
      "4091/4091 [==============================] - 1s 335us/sample - loss: 75.4550 - val_loss: 76.0948\n",
      "Epoch 57/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 80.5404 - val_loss: 84.5450\n",
      "Epoch 58/1000\n",
      "4091/4091 [==============================] - 1s 346us/sample - loss: 73.2041 - val_loss: 73.0237\n",
      "Epoch 59/1000\n",
      "4091/4091 [==============================] - 1s 342us/sample - loss: 73.4647 - val_loss: 72.1101\n",
      "Epoch 60/1000\n",
      "4091/4091 [==============================] - 1s 347us/sample - loss: 73.1034 - val_loss: 71.9031\n",
      "Epoch 61/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 71.2015 - val_loss: 83.4029\n",
      "Epoch 62/1000\n",
      "4091/4091 [==============================] - 1s 344us/sample - loss: 70.7308 - val_loss: 70.4165\n",
      "Epoch 63/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 72.1787 - val_loss: 74.9217\n",
      "Epoch 64/1000\n",
      "4091/4091 [==============================] - 1s 316us/sample - loss: 72.2443 - val_loss: 100.8458\n",
      "Epoch 65/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 70.1438 - val_loss: 87.9933\n",
      "Epoch 66/1000\n",
      "4091/4091 [==============================] - 1s 340us/sample - loss: 70.5164 - val_loss: 69.9062\n",
      "Epoch 67/1000\n",
      "4091/4091 [==============================] - 1s 340us/sample - loss: 68.5042 - val_loss: 68.8111\n",
      "Epoch 68/1000\n",
      "4091/4091 [==============================] - 1s 349us/sample - loss: 74.6534 - val_loss: 67.5276\n",
      "Epoch 69/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 67.6159 - val_loss: 76.1815\n",
      "Epoch 70/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 66.6208 - val_loss: 74.3147\n",
      "Epoch 71/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 69.6999 - val_loss: 69.7301\n",
      "Epoch 72/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 65.3418 - val_loss: 70.0215\n",
      "Epoch 73/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 65.1148 - val_loss: 72.1779\n",
      "Epoch 74/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 65.9133 - val_loss: 81.3236\n",
      "Epoch 75/1000\n",
      "4091/4091 [==============================] - 1s 349us/sample - loss: 65.8467 - val_loss: 67.1376\n",
      "Epoch 76/1000\n",
      "4091/4091 [==============================] - 1s 346us/sample - loss: 66.4460 - val_loss: 64.7855\n",
      "Epoch 77/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 65.3503 - val_loss: 86.2608\n",
      "Epoch 78/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 67.1698 - val_loss: 65.0108\n",
      "Epoch 79/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 64.4753 - val_loss: 66.1971\n",
      "Epoch 80/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 62.5609 - val_loss: 78.4206\n",
      "Epoch 81/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 62.3289 - val_loss: 66.9998\n",
      "Epoch 82/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 61.6876 - val_loss: 68.5492\n",
      "Epoch 83/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 66.4755 - val_loss: 66.2271\n",
      "Epoch 84/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 66.1058 - val_loss: 75.2561\n",
      "Epoch 85/1000\n",
      "4091/4091 [==============================] - 1s 348us/sample - loss: 66.9600 - val_loss: 63.5580\n",
      "Epoch 86/1000\n",
      "4091/4091 [==============================] - 1s 345us/sample - loss: 66.2341 - val_loss: 61.1822\n",
      "Epoch 87/1000\n",
      "4091/4091 [==============================] - 1s 314us/sample - loss: 63.5908 - val_loss: 62.9136\n",
      "Epoch 88/1000\n",
      "4091/4091 [==============================] - 1s 348us/sample - loss: 62.8744 - val_loss: 59.9414\n",
      "Epoch 89/1000\n",
      "4091/4091 [==============================] - 1s 318us/sample - loss: 61.9590 - val_loss: 62.3644\n",
      "Epoch 90/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 64.4922 - val_loss: 65.0404\n",
      "Epoch 91/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 58.7122 - val_loss: 62.8390\n",
      "Epoch 92/1000\n",
      "4091/4091 [==============================] - 1s 317us/sample - loss: 58.1099 - val_loss: 73.1293\n",
      "Epoch 93/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 65.8411 - val_loss: 62.3218\n",
      "Epoch 94/1000\n",
      "4091/4091 [==============================] - 1s 318us/sample - loss: 60.7222 - val_loss: 63.2456\n",
      "Epoch 95/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 61.1681 - val_loss: 66.5196\n",
      "Epoch 96/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 62.3872 - val_loss: 82.5516\n",
      "Epoch 97/1000\n",
      "4091/4091 [==============================] - 1s 349us/sample - loss: 61.1925 - val_loss: 59.4811\n",
      "Epoch 98/1000\n",
      "4091/4091 [==============================] - 1s 347us/sample - loss: 60.5950 - val_loss: 57.6411\n",
      "Epoch 99/1000\n",
      "4091/4091 [==============================] - 1s 338us/sample - loss: 58.3865 - val_loss: 62.1639\n",
      "Epoch 100/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 59.6248 - val_loss: 83.3169\n",
      "Epoch 101/1000\n",
      "4091/4091 [==============================] - 1s 347us/sample - loss: 58.9626 - val_loss: 56.6742\n",
      "Epoch 102/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 58.1775 - val_loss: 60.2397\n",
      "Epoch 103/1000\n",
      "4091/4091 [==============================] - 1s 342us/sample - loss: 56.1792 - val_loss: 55.0886\n",
      "Epoch 104/1000\n",
      "4091/4091 [==============================] - 1s 317us/sample - loss: 58.0916 - val_loss: 66.4643\n",
      "Epoch 105/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 56.8823 - val_loss: 61.1034\n",
      "Epoch 106/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 58.0142 - val_loss: 59.5355\n",
      "Epoch 107/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 55.0849 - val_loss: 63.7222\n",
      "Epoch 108/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 55.0296 - val_loss: 56.9658\n",
      "Epoch 109/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 56.6401 - val_loss: 63.2411\n",
      "Epoch 110/1000\n",
      "4091/4091 [==============================] - 1s 345us/sample - loss: 60.7159 - val_loss: 54.8662\n",
      "Epoch 111/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 56.4605 - val_loss: 62.6434\n",
      "Epoch 112/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 54.8144 - val_loss: 67.3474\n",
      "Epoch 113/1000\n",
      "4091/4091 [==============================] - 1s 318us/sample - loss: 55.7636 - val_loss: 57.8862\n",
      "Epoch 114/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 57.5755 - val_loss: 55.7059\n",
      "Epoch 115/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 54.5888 - val_loss: 73.1556\n",
      "Epoch 116/1000\n",
      "4091/4091 [==============================] - 1s 329us/sample - loss: 56.3807 - val_loss: 62.9253\n",
      "Epoch 117/1000\n",
      "4091/4091 [==============================] - 1s 336us/sample - loss: 55.1713 - val_loss: 63.1862\n",
      "Epoch 118/1000\n",
      "4091/4091 [==============================] - 1s 345us/sample - loss: 54.3485 - val_loss: 53.6158\n",
      "Epoch 119/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 58.0745 - val_loss: 72.3787\n",
      "Epoch 120/1000\n",
      "4091/4091 [==============================] - 1s 342us/sample - loss: 53.2474 - val_loss: 53.2981\n",
      "Epoch 121/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 53.3324 - val_loss: 60.0202\n",
      "Epoch 122/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 56.1345 - val_loss: 56.6539\n",
      "Epoch 123/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 55.6048 - val_loss: 53.8470\n",
      "Epoch 124/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 55.4406 - val_loss: 53.9216\n",
      "Epoch 125/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 54.9159 - val_loss: 56.9019\n",
      "Epoch 126/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 53.1137 - val_loss: 57.9760\n",
      "Epoch 127/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 55.7312 - val_loss: 55.7848\n",
      "Epoch 128/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 53.7925 - val_loss: 55.8201\n",
      "Epoch 129/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 54.8159 - val_loss: 59.2769\n",
      "Epoch 130/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 56.9402 - val_loss: 62.3393\n",
      "Epoch 131/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 54.3375 - val_loss: 60.8302\n",
      "Epoch 132/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 54.0189 - val_loss: 57.9956\n",
      "Epoch 133/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 52.4638 - val_loss: 63.1204\n",
      "Epoch 134/1000\n",
      "4091/4091 [==============================] - 1s 345us/sample - loss: 53.3648 - val_loss: 51.1443\n",
      "Epoch 135/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 51.0324 - val_loss: 60.6306\n",
      "Epoch 136/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 51.9262 - val_loss: 62.1811\n",
      "Epoch 137/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 52.1521 - val_loss: 51.8896\n",
      "Epoch 138/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 53.0467 - val_loss: 51.9743\n",
      "Epoch 139/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 54.6146 - val_loss: 53.1602\n",
      "Epoch 140/1000\n",
      "4091/4091 [==============================] - 1s 318us/sample - loss: 50.2313 - val_loss: 55.8959\n",
      "Epoch 141/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 52.2591 - val_loss: 55.5872\n",
      "Epoch 142/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 51.0106 - val_loss: 55.2469\n",
      "Epoch 143/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 49.5902 - val_loss: 59.2508\n",
      "Epoch 144/1000\n",
      "4091/4091 [==============================] - 1s 347us/sample - loss: 51.0734 - val_loss: 49.3186\n",
      "Epoch 145/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 52.8240 - val_loss: 49.8925\n",
      "Epoch 146/1000\n",
      "4091/4091 [==============================] - 1s 318us/sample - loss: 49.8383 - val_loss: 60.1863\n",
      "Epoch 147/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 49.8131 - val_loss: 51.8403\n",
      "Epoch 148/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 54.9657 - val_loss: 58.0174\n",
      "Epoch 149/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 53.3241 - val_loss: 64.1129\n",
      "Epoch 150/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 51.8230 - val_loss: 57.7992\n",
      "Epoch 151/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 47.1740 - val_loss: 52.9786\n",
      "Epoch 152/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 51.4712 - val_loss: 50.1220\n",
      "Epoch 153/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 52.2904 - val_loss: 57.6687\n",
      "Epoch 154/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 49.0401 - val_loss: 50.4474\n",
      "Epoch 155/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 49.7739 - val_loss: 60.7927\n",
      "Epoch 156/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 52.4839 - val_loss: 54.8574\n",
      "Epoch 157/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 51.9675 - val_loss: 53.0006\n",
      "Epoch 158/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 48.6714 - val_loss: 53.8974\n",
      "Epoch 159/1000\n",
      "4091/4091 [==============================] - 1s 318us/sample - loss: 49.7368 - val_loss: 53.4207\n",
      "Epoch 160/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 47.9995 - val_loss: 54.7598\n",
      "Epoch 161/1000\n",
      "4091/4091 [==============================] - 1s 317us/sample - loss: 47.2434 - val_loss: 49.3312\n",
      "Epoch 162/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 47.8070 - val_loss: 49.6630\n",
      "Epoch 163/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 50.5355 - val_loss: 50.1620\n",
      "Epoch 164/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 50.7086 - val_loss: 61.0108\n",
      "Epoch 165/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 49.3449 - val_loss: 50.3502\n",
      "Epoch 166/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 48.4181 - val_loss: 53.0648\n",
      "Epoch 167/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 49.3412 - val_loss: 54.8510\n",
      "Epoch 168/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 49.7110 - val_loss: 52.5370\n",
      "Epoch 169/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 50.6723 - val_loss: 49.7404\n",
      "Epoch 170/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 47.7853 - val_loss: 51.2346\n",
      "Epoch 171/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 48.0793 - val_loss: 53.5197\n",
      "Epoch 172/1000\n",
      "4091/4091 [==============================] - 1s 347us/sample - loss: 49.5234 - val_loss: 48.4679\n",
      "Epoch 173/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 46.5135 - val_loss: 53.1096\n",
      "Epoch 174/1000\n",
      "4091/4091 [==============================] - 1s 343us/sample - loss: 45.8574 - val_loss: 47.9413\n",
      "Epoch 175/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 47.3903 - val_loss: 51.7425\n",
      "Epoch 176/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 50.1108 - val_loss: 65.6904\n",
      "Epoch 177/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 49.2022 - val_loss: 50.1333\n",
      "Epoch 178/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 45.9212 - val_loss: 55.4720\n",
      "Epoch 179/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 47.6589 - val_loss: 68.8221\n",
      "Epoch 180/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 47.5107 - val_loss: 55.6355\n",
      "Epoch 181/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 46.8582 - val_loss: 59.1039\n",
      "Epoch 182/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 47.5404 - val_loss: 53.7267\n",
      "Epoch 183/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 45.9829 - val_loss: 53.8038\n",
      "Epoch 184/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 46.5544 - val_loss: 55.1229\n",
      "Epoch 185/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 48.5947 - val_loss: 48.6798\n",
      "Epoch 186/1000\n",
      "4091/4091 [==============================] - 1s 348us/sample - loss: 47.2518 - val_loss: 47.3474\n",
      "Epoch 187/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 47.0932 - val_loss: 53.8831\n",
      "Epoch 188/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 43.8231 - val_loss: 57.3401\n",
      "Epoch 189/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 48.5585 - val_loss: 54.6572\n",
      "Epoch 190/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 45.1023 - val_loss: 54.3584\n",
      "Epoch 191/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 46.1394 - val_loss: 53.2374\n",
      "Epoch 192/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 45.3911 - val_loss: 51.5363\n",
      "Epoch 193/1000\n",
      "4091/4091 [==============================] - 1s 346us/sample - loss: 45.8364 - val_loss: 46.3034\n",
      "Epoch 194/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 49.2665 - val_loss: 50.9745\n",
      "Epoch 195/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 43.5699 - val_loss: 49.1528\n",
      "Epoch 196/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 45.7290 - val_loss: 47.9428\n",
      "Epoch 197/1000\n",
      "4091/4091 [==============================] - 1s 317us/sample - loss: 45.4167 - val_loss: 49.1829\n",
      "Epoch 198/1000\n",
      "4091/4091 [==============================] - 1s 318us/sample - loss: 45.4889 - val_loss: 52.2976\n",
      "Epoch 199/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 44.7221 - val_loss: 47.6856\n",
      "Epoch 200/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 45.9615 - val_loss: 52.2812\n",
      "Epoch 201/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 46.3333 - val_loss: 48.0067\n",
      "Epoch 202/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 44.4365 - val_loss: 48.9063\n",
      "Epoch 203/1000\n",
      "4091/4091 [==============================] - 1s 348us/sample - loss: 47.0370 - val_loss: 45.9615\n",
      "Epoch 204/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 45.1750 - val_loss: 47.8067\n",
      "Epoch 205/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 45.8231 - val_loss: 54.5314\n",
      "Epoch 206/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 46.1940 - val_loss: 52.8203\n",
      "Epoch 207/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 44.5961 - val_loss: 63.4212\n",
      "Epoch 208/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 43.1076 - val_loss: 62.4342\n",
      "Epoch 209/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 42.3982 - val_loss: 45.9663\n",
      "Epoch 210/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 43.9398 - val_loss: 46.5424\n",
      "Epoch 211/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 44.5864 - val_loss: 52.6839\n",
      "Epoch 212/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 42.5453 - val_loss: 55.2175\n",
      "Epoch 213/1000\n",
      "4091/4091 [==============================] - 1s 316us/sample - loss: 43.4486 - val_loss: 50.0201\n",
      "Epoch 214/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 44.7233 - val_loss: 64.3895\n",
      "Epoch 215/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 45.3821 - val_loss: 48.1742\n",
      "Epoch 216/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 44.5291 - val_loss: 53.4008\n",
      "Epoch 217/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 43.6700 - val_loss: 57.8775\n",
      "Epoch 218/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 43.8909 - val_loss: 48.0030\n",
      "Epoch 219/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 42.3346 - val_loss: 46.4790\n",
      "Epoch 220/1000\n",
      "4091/4091 [==============================] - 1s 345us/sample - loss: 47.5063 - val_loss: 45.9484\n",
      "Epoch 221/1000\n",
      "4091/4091 [==============================] - 1s 343us/sample - loss: 42.9288 - val_loss: 44.5671\n",
      "Epoch 222/1000\n",
      "4091/4091 [==============================] - 1s 334us/sample - loss: 41.8065 - val_loss: 46.8623\n",
      "Epoch 223/1000\n",
      "4091/4091 [==============================] - 1s 333us/sample - loss: 43.0306 - val_loss: 44.7547\n",
      "Epoch 224/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 42.1961 - val_loss: 46.5826\n",
      "Epoch 225/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 44.7357 - val_loss: 61.7759\n",
      "Epoch 226/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 43.3879 - val_loss: 48.8887\n",
      "Epoch 227/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 42.8515 - val_loss: 47.3563\n",
      "Epoch 228/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 45.8096 - val_loss: 57.6167\n",
      "Epoch 229/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 44.0557 - val_loss: 46.6094\n",
      "Epoch 230/1000\n",
      "4091/4091 [==============================] - 1s 348us/sample - loss: 41.3034 - val_loss: 44.5420\n",
      "Epoch 231/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 42.4911 - val_loss: 50.0339\n",
      "Epoch 232/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 42.0350 - val_loss: 47.5309\n",
      "Epoch 233/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 43.1573 - val_loss: 57.1820\n",
      "Epoch 234/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 43.1671 - val_loss: 51.1307\n",
      "Epoch 235/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 43.6302 - val_loss: 47.8921\n",
      "Epoch 236/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 44.6221 - val_loss: 56.7510\n",
      "Epoch 237/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 42.3000 - val_loss: 47.4208\n",
      "Epoch 238/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 42.6694 - val_loss: 46.9461\n",
      "Epoch 239/1000\n",
      "4091/4091 [==============================] - 1s 332us/sample - loss: 41.7391 - val_loss: 46.6638\n",
      "Epoch 240/1000\n",
      "4091/4091 [==============================] - 1s 318us/sample - loss: 40.8950 - val_loss: 45.5119\n",
      "Epoch 241/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 43.9447 - val_loss: 51.8326\n",
      "Epoch 242/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 44.9342 - val_loss: 46.3003\n",
      "Epoch 243/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 41.5911 - val_loss: 46.6119\n",
      "Epoch 244/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 41.7107 - val_loss: 52.7497\n",
      "Epoch 245/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 40.9088 - val_loss: 47.2852\n",
      "Epoch 246/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 42.1021 - val_loss: 49.2100\n",
      "Epoch 247/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 40.8304 - val_loss: 55.6292\n",
      "Epoch 248/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 41.6124 - val_loss: 45.6459\n",
      "Epoch 249/1000\n",
      "4091/4091 [==============================] - 1s 347us/sample - loss: 43.7730 - val_loss: 44.1206\n",
      "Epoch 250/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 42.5629 - val_loss: 60.2833\n",
      "Epoch 251/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 39.6061 - val_loss: 45.8809\n",
      "Epoch 252/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 42.3875 - val_loss: 44.2837\n",
      "Epoch 253/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 39.4645 - val_loss: 57.6481\n",
      "Epoch 254/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 40.4620 - val_loss: 49.2737\n",
      "Epoch 255/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 44.6016 - val_loss: 56.0337\n",
      "Epoch 256/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 40.6697 - val_loss: 46.9944\n",
      "Epoch 257/1000\n",
      "4091/4091 [==============================] - 1s 344us/sample - loss: 41.2466 - val_loss: 42.0530\n",
      "Epoch 258/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 40.1657 - val_loss: 46.6283\n",
      "Epoch 259/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 39.7477 - val_loss: 50.1441\n",
      "Epoch 260/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 40.8975 - val_loss: 44.8169\n",
      "Epoch 261/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 40.5291 - val_loss: 43.3944\n",
      "Epoch 262/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 42.7410 - val_loss: 54.7485\n",
      "Epoch 263/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 41.0396 - val_loss: 43.6092\n",
      "Epoch 264/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 37.5485 - val_loss: 46.1721\n",
      "Epoch 265/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 40.7313 - val_loss: 45.4980\n",
      "Epoch 266/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 38.6519 - val_loss: 44.0953\n",
      "Epoch 267/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 37.9528 - val_loss: 47.3637\n",
      "Epoch 268/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 39.0218 - val_loss: 50.6565\n",
      "Epoch 269/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 39.0528 - val_loss: 50.5081\n",
      "Epoch 270/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 39.7070 - val_loss: 47.6862\n",
      "Epoch 271/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 40.3986 - val_loss: 48.0163\n",
      "Epoch 272/1000\n",
      "4091/4091 [==============================] - 1s 344us/sample - loss: 38.4146 - val_loss: 41.8596\n",
      "Epoch 273/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 38.3241 - val_loss: 47.4344\n",
      "Epoch 274/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 41.0310 - val_loss: 45.9921\n",
      "Epoch 275/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 42.6486 - val_loss: 42.3650\n",
      "Epoch 276/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 40.0034 - val_loss: 51.1559\n",
      "Epoch 277/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 41.2784 - val_loss: 46.0332\n",
      "Epoch 278/1000\n",
      "4091/4091 [==============================] - 1s 348us/sample - loss: 39.0032 - val_loss: 40.6737\n",
      "Epoch 279/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 40.2405 - val_loss: 44.4588\n",
      "Epoch 280/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 40.4562 - val_loss: 49.5045\n",
      "Epoch 281/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 38.9454 - val_loss: 59.4928\n",
      "Epoch 282/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 39.9173 - val_loss: 42.3575\n",
      "Epoch 283/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 37.9999 - val_loss: 45.1160\n",
      "Epoch 284/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 40.5169 - val_loss: 52.7440\n",
      "Epoch 285/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 41.0443 - val_loss: 41.6036\n",
      "Epoch 286/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 38.6443 - val_loss: 44.1771\n",
      "Epoch 287/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 39.3426 - val_loss: 41.2624\n",
      "Epoch 288/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 38.1723 - val_loss: 44.8250\n",
      "Epoch 289/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 37.0389 - val_loss: 61.3440\n",
      "Epoch 290/1000\n",
      "4091/4091 [==============================] - 1s 335us/sample - loss: 38.9802 - val_loss: 51.5557\n",
      "Epoch 291/1000\n",
      "4091/4091 [==============================] - 1s 343us/sample - loss: 39.3068 - val_loss: 46.8487\n",
      "Epoch 292/1000\n",
      "4091/4091 [==============================] - 1s 340us/sample - loss: 38.4039 - val_loss: 43.9876\n",
      "Epoch 293/1000\n",
      "4091/4091 [==============================] - 1s 317us/sample - loss: 38.5177 - val_loss: 42.1151\n",
      "Epoch 294/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 39.3506 - val_loss: 44.5291\n",
      "Epoch 295/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 36.0708 - val_loss: 49.2599\n",
      "Epoch 296/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 36.4927 - val_loss: 44.9032\n",
      "Epoch 297/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 38.5399 - val_loss: 41.8626\n",
      "Epoch 298/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 37.1009 - val_loss: 54.5879\n",
      "Epoch 299/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 40.0832 - val_loss: 41.4755\n",
      "Epoch 300/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 38.4380 - val_loss: 51.5889\n",
      "Epoch 301/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 37.8854 - val_loss: 43.5141\n",
      "Epoch 302/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 35.6261 - val_loss: 44.3342\n",
      "Epoch 303/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 40.4395 - val_loss: 45.5554\n",
      "Epoch 304/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 37.0730 - val_loss: 42.9030\n",
      "Epoch 305/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 36.9510 - val_loss: 42.6433\n",
      "Epoch 306/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 36.5380 - val_loss: 43.1242\n",
      "Epoch 307/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 37.5237 - val_loss: 44.4057\n",
      "Epoch 308/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 37.1469 - val_loss: 42.3244\n",
      "Epoch 309/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 37.9717 - val_loss: 55.9461\n",
      "Epoch 310/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 38.9591 - val_loss: 41.6208\n",
      "Epoch 311/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 38.7754 - val_loss: 51.5966\n",
      "Epoch 312/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 38.0564 - val_loss: 53.9523\n",
      "Epoch 313/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 36.0877 - val_loss: 43.5198\n",
      "Epoch 314/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 36.4476 - val_loss: 43.9481\n",
      "Epoch 315/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 36.1007 - val_loss: 47.4050\n",
      "Epoch 316/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 37.0737 - val_loss: 48.5797\n",
      "Epoch 317/1000\n",
      "4091/4091 [==============================] - 1s 350us/sample - loss: 37.0109 - val_loss: 40.5806\n",
      "Epoch 318/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 36.2912 - val_loss: 40.8152\n",
      "Epoch 319/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 36.1463 - val_loss: 41.7025\n",
      "Epoch 320/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 40.0306 - val_loss: 44.5750\n",
      "Epoch 321/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 38.5491 - val_loss: 40.7350\n",
      "Epoch 322/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 37.2661 - val_loss: 42.3642\n",
      "Epoch 323/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 36.1734 - val_loss: 42.8711\n",
      "Epoch 324/1000\n",
      "4091/4091 [==============================] - 1s 352us/sample - loss: 37.9029 - val_loss: 40.4263\n",
      "Epoch 325/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 35.9540 - val_loss: 43.2749\n",
      "Epoch 326/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 36.5320 - val_loss: 52.3191\n",
      "Epoch 327/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 36.9786 - val_loss: 45.6161\n",
      "Epoch 328/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 37.4069 - val_loss: 41.9919\n",
      "Epoch 329/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 35.7734 - val_loss: 40.9915\n",
      "Epoch 330/1000\n",
      "4091/4091 [==============================] - 1s 317us/sample - loss: 37.3028 - val_loss: 41.1583\n",
      "Epoch 331/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 38.2289 - val_loss: 42.8638\n",
      "Epoch 332/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 37.3662 - val_loss: 40.7947\n",
      "Epoch 333/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 36.3563 - val_loss: 41.3046\n",
      "Epoch 334/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 34.3150 - val_loss: 43.7291\n",
      "Epoch 335/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 37.5110 - val_loss: 40.8898\n",
      "Epoch 336/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 35.9025 - val_loss: 42.0597\n",
      "Epoch 337/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 36.0839 - val_loss: 40.9665\n",
      "Epoch 338/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 35.0579 - val_loss: 41.5361\n",
      "Epoch 339/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 36.0218 - val_loss: 42.1136\n",
      "Epoch 340/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 35.3435 - val_loss: 41.7397\n",
      "Epoch 341/1000\n",
      "4091/4091 [==============================] - 1s 346us/sample - loss: 34.5706 - val_loss: 40.1388\n",
      "Epoch 342/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 35.4836 - val_loss: 44.0659\n",
      "Epoch 343/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 36.6753 - val_loss: 43.5613\n",
      "Epoch 344/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 36.5622 - val_loss: 43.1508\n",
      "Epoch 345/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 35.8858 - val_loss: 54.8436\n",
      "Epoch 346/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 36.6597 - val_loss: 41.9259\n",
      "Epoch 347/1000\n",
      "4091/4091 [==============================] - 1s 348us/sample - loss: 35.9910 - val_loss: 39.9143\n",
      "Epoch 348/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 34.6670 - val_loss: 41.6076\n",
      "Epoch 349/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 37.5188 - val_loss: 40.4083\n",
      "Epoch 350/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 37.0636 - val_loss: 40.3923\n",
      "Epoch 351/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 34.9504 - val_loss: 42.3894\n",
      "Epoch 352/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 34.2430 - val_loss: 41.7150\n",
      "Epoch 353/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 34.1108 - val_loss: 42.3284\n",
      "Epoch 354/1000\n",
      "4091/4091 [==============================] - 1s 318us/sample - loss: 36.1258 - val_loss: 48.5945\n",
      "Epoch 355/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 34.7755 - val_loss: 46.4160\n",
      "Epoch 356/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 37.5151 - val_loss: 41.0707\n",
      "Epoch 357/1000\n",
      "4091/4091 [==============================] - 1s 343us/sample - loss: 34.7265 - val_loss: 39.4118\n",
      "Epoch 358/1000\n",
      "4091/4091 [==============================] - 1s 340us/sample - loss: 34.5869 - val_loss: 41.9150\n",
      "Epoch 359/1000\n",
      "4091/4091 [==============================] - 1s 338us/sample - loss: 35.7253 - val_loss: 43.8245\n",
      "Epoch 360/1000\n",
      "4091/4091 [==============================] - 1s 332us/sample - loss: 35.4301 - val_loss: 40.8939\n",
      "Epoch 361/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 35.4955 - val_loss: 40.3159\n",
      "Epoch 362/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 32.7288 - val_loss: 40.5591\n",
      "Epoch 363/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 35.5856 - val_loss: 50.6868\n",
      "Epoch 364/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 36.8186 - val_loss: 47.0251\n",
      "Epoch 365/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 35.5894 - val_loss: 44.9750\n",
      "Epoch 366/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 36.0046 - val_loss: 40.4387\n",
      "Epoch 367/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 35.2687 - val_loss: 42.8120\n",
      "Epoch 368/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 35.2483 - val_loss: 44.0900\n",
      "Epoch 369/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 36.0570 - val_loss: 46.3082\n",
      "Epoch 370/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 34.4894 - val_loss: 41.9852\n",
      "Epoch 371/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 34.3057 - val_loss: 43.4923\n",
      "Epoch 372/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 35.7655 - val_loss: 49.5842\n",
      "Epoch 373/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 35.7176 - val_loss: 41.0357\n",
      "Epoch 374/1000\n",
      "4091/4091 [==============================] - 1s 359us/sample - loss: 34.4190 - val_loss: 39.1243\n",
      "Epoch 375/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 36.0216 - val_loss: 53.3433\n",
      "Epoch 376/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 36.0505 - val_loss: 44.4725\n",
      "Epoch 377/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 33.8610 - val_loss: 44.4052\n",
      "Epoch 378/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 38.4972 - val_loss: 40.4341\n",
      "Epoch 379/1000\n",
      "4091/4091 [==============================] - 1s 349us/sample - loss: 35.4921 - val_loss: 38.5784\n",
      "Epoch 380/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 34.4808 - val_loss: 39.7280\n",
      "Epoch 381/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 33.4626 - val_loss: 41.0317\n",
      "Epoch 382/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 34.1565 - val_loss: 44.7473\n",
      "Epoch 383/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 36.1436 - val_loss: 39.4113\n",
      "Epoch 384/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 36.0620 - val_loss: 43.1304\n",
      "Epoch 385/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 32.5108 - val_loss: 45.1579\n",
      "Epoch 386/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 33.0703 - val_loss: 38.7828\n",
      "Epoch 387/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 34.1624 - val_loss: 47.0428\n",
      "Epoch 388/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 35.8656 - val_loss: 44.6866\n",
      "Epoch 389/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 34.4983 - val_loss: 39.9178\n",
      "Epoch 390/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 38.0859 - val_loss: 44.4520\n",
      "Epoch 391/1000\n",
      "4091/4091 [==============================] - 1s 345us/sample - loss: 35.6070 - val_loss: 38.3754\n",
      "Epoch 392/1000\n",
      "4091/4091 [==============================] - 1s 329us/sample - loss: 32.7487 - val_loss: 43.5099\n",
      "Epoch 393/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 35.4999 - val_loss: 47.0851\n",
      "Epoch 394/1000\n",
      "4091/4091 [==============================] - 1s 340us/sample - loss: 33.7941 - val_loss: 40.8774\n",
      "Epoch 395/1000\n",
      "4091/4091 [==============================] - 1s 342us/sample - loss: 36.5012 - val_loss: 41.7536\n",
      "Epoch 396/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 33.0353 - val_loss: 42.4937\n",
      "Epoch 397/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 35.1370 - val_loss: 42.3961\n",
      "Epoch 398/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 34.2531 - val_loss: 39.6011\n",
      "Epoch 399/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 31.8962 - val_loss: 44.7630\n",
      "Epoch 400/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 34.3348 - val_loss: 41.2984\n",
      "Epoch 401/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 32.9832 - val_loss: 43.1240\n",
      "Epoch 402/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 33.4247 - val_loss: 41.7002\n",
      "Epoch 403/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 31.9799 - val_loss: 57.3877\n",
      "Epoch 404/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 33.4443 - val_loss: 43.4312\n",
      "Epoch 405/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 35.7391 - val_loss: 48.3547\n",
      "Epoch 406/1000\n",
      "4091/4091 [==============================] - 1s 353us/sample - loss: 33.4554 - val_loss: 38.3034\n",
      "Epoch 407/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 33.0203 - val_loss: 43.6688\n",
      "Epoch 408/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 33.2995 - val_loss: 39.1291\n",
      "Epoch 409/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 32.0292 - val_loss: 40.0410\n",
      "Epoch 410/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 35.2255 - val_loss: 40.3991\n",
      "Epoch 411/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 34.2053 - val_loss: 43.3580\n",
      "Epoch 412/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 32.6412 - val_loss: 41.1791\n",
      "Epoch 413/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 33.4209 - val_loss: 39.2394\n",
      "Epoch 414/1000\n",
      "4091/4091 [==============================] - 1s 344us/sample - loss: 34.5862 - val_loss: 37.9083\n",
      "Epoch 415/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 31.1009 - val_loss: 43.2566\n",
      "Epoch 416/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 33.0102 - val_loss: 40.3451\n",
      "Epoch 417/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 31.5653 - val_loss: 45.7984\n",
      "Epoch 418/1000\n",
      "4091/4091 [==============================] - 1s 337us/sample - loss: 33.7458 - val_loss: 38.8060\n",
      "Epoch 419/1000\n",
      "4091/4091 [==============================] - 1s 331us/sample - loss: 37.0853 - val_loss: 41.6774\n",
      "Epoch 420/1000\n",
      "4091/4091 [==============================] - 1s 331us/sample - loss: 32.6392 - val_loss: 43.3143\n",
      "Epoch 421/1000\n",
      "4091/4091 [==============================] - 1s 329us/sample - loss: 35.9390 - val_loss: 38.9060\n",
      "Epoch 422/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 32.6075 - val_loss: 45.5207\n",
      "Epoch 423/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 32.7543 - val_loss: 47.4139\n",
      "Epoch 424/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 33.4164 - val_loss: 40.6977\n",
      "Epoch 425/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 34.8063 - val_loss: 40.4968\n",
      "Epoch 426/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 33.3857 - val_loss: 39.5751\n",
      "Epoch 427/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 31.3212 - val_loss: 40.7071\n",
      "Epoch 428/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 34.2548 - val_loss: 52.1354\n",
      "Epoch 429/1000\n",
      "4091/4091 [==============================] - 1s 332us/sample - loss: 34.2724 - val_loss: 46.3909\n",
      "Epoch 430/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 33.3629 - val_loss: 38.2354\n",
      "Epoch 431/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 31.6873 - val_loss: 39.5875\n",
      "Epoch 432/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 32.4500 - val_loss: 40.1543\n",
      "Epoch 433/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 32.5484 - val_loss: 39.5938\n",
      "Epoch 434/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 31.8857 - val_loss: 41.1395\n",
      "Epoch 435/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 31.8678 - val_loss: 42.4351\n",
      "Epoch 436/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 35.0067 - val_loss: 42.1828\n",
      "Epoch 437/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 30.6976 - val_loss: 42.4374\n",
      "Epoch 438/1000\n",
      "4091/4091 [==============================] - 1s 349us/sample - loss: 34.6033 - val_loss: 37.8727\n",
      "Epoch 439/1000\n",
      "4091/4091 [==============================] - 1s 357us/sample - loss: 30.8675 - val_loss: 37.6818\n",
      "Epoch 440/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 32.4956 - val_loss: 46.0914\n",
      "Epoch 441/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 31.8876 - val_loss: 40.8824\n",
      "Epoch 442/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 29.9051 - val_loss: 38.0952\n",
      "Epoch 443/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 31.3628 - val_loss: 38.7371\n",
      "Epoch 444/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 33.4594 - val_loss: 43.4494\n",
      "Epoch 445/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 30.9616 - val_loss: 43.0747\n",
      "Epoch 446/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 32.2761 - val_loss: 42.1433\n",
      "Epoch 447/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 32.3092 - val_loss: 41.7866\n",
      "Epoch 448/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 31.7244 - val_loss: 39.1763\n",
      "Epoch 449/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 31.0054 - val_loss: 49.3208\n",
      "Epoch 450/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 31.8610 - val_loss: 39.8611\n",
      "Epoch 451/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 33.3339 - val_loss: 44.7707\n",
      "Epoch 452/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 32.6660 - val_loss: 43.4284\n",
      "Epoch 453/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 32.5674 - val_loss: 45.2388\n",
      "Epoch 454/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 31.5859 - val_loss: 41.1227\n",
      "Epoch 455/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 31.8929 - val_loss: 48.9678\n",
      "Epoch 456/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 32.8730 - val_loss: 38.2359\n",
      "Epoch 457/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 32.1259 - val_loss: 43.7979\n",
      "Epoch 458/1000\n",
      "4091/4091 [==============================] - 1s 351us/sample - loss: 30.6585 - val_loss: 36.2855\n",
      "Epoch 459/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 32.2582 - val_loss: 40.1033\n",
      "Epoch 460/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 32.3012 - val_loss: 39.6353\n",
      "Epoch 461/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 31.2744 - val_loss: 42.3203\n",
      "Epoch 462/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 31.9717 - val_loss: 38.8082\n",
      "Epoch 463/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 31.5029 - val_loss: 39.8066\n",
      "Epoch 464/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 33.7089 - val_loss: 37.9689\n",
      "Epoch 465/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 31.5387 - val_loss: 37.1325\n",
      "Epoch 466/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 31.3881 - val_loss: 41.3327\n",
      "Epoch 467/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 30.9016 - val_loss: 39.0388\n",
      "Epoch 468/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 31.2162 - val_loss: 47.1368\n",
      "Epoch 469/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 32.2427 - val_loss: 48.7293\n",
      "Epoch 470/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 30.1270 - val_loss: 39.1725\n",
      "Epoch 471/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 30.7807 - val_loss: 38.5475\n",
      "Epoch 472/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 31.1562 - val_loss: 38.8121\n",
      "Epoch 473/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 30.2168 - val_loss: 42.9200\n",
      "Epoch 474/1000\n",
      "4091/4091 [==============================] - 1s 333us/sample - loss: 34.0759 - val_loss: 44.5507\n",
      "Epoch 475/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 31.4047 - val_loss: 41.4743\n",
      "Epoch 476/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 31.1293 - val_loss: 38.6652\n",
      "Epoch 477/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 29.4970 - val_loss: 42.7854\n",
      "Epoch 478/1000\n",
      "4091/4091 [==============================] - 1s 319us/sample - loss: 30.1185 - val_loss: 37.9726\n",
      "Epoch 479/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 31.0122 - val_loss: 41.0374\n",
      "Epoch 480/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 31.4068 - val_loss: 40.7415\n",
      "Epoch 481/1000\n",
      "4091/4091 [==============================] - 1s 351us/sample - loss: 32.0135 - val_loss: 36.1402\n",
      "Epoch 482/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 31.9357 - val_loss: 43.1245\n",
      "Epoch 483/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 30.4340 - val_loss: 37.9817\n",
      "Epoch 484/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 30.1578 - val_loss: 38.2438\n",
      "Epoch 485/1000\n",
      "4091/4091 [==============================] - 1s 331us/sample - loss: 29.2445 - val_loss: 40.4670\n",
      "Epoch 486/1000\n",
      "4091/4091 [==============================] - 1s 329us/sample - loss: 29.3676 - val_loss: 38.5631\n",
      "Epoch 487/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 31.0713 - val_loss: 38.2539\n",
      "Epoch 488/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 31.5046 - val_loss: 47.2713\n",
      "Epoch 489/1000\n",
      "4091/4091 [==============================] - 1s 318us/sample - loss: 31.0340 - val_loss: 36.9048\n",
      "Epoch 490/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 30.8418 - val_loss: 41.3963\n",
      "Epoch 491/1000\n",
      "4091/4091 [==============================] - 1s 329us/sample - loss: 30.4205 - val_loss: 36.9002\n",
      "Epoch 492/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 32.0829 - val_loss: 37.0317\n",
      "Epoch 493/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 30.0137 - val_loss: 37.8975\n",
      "Epoch 494/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 29.4998 - val_loss: 38.1553\n",
      "Epoch 495/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 32.0257 - val_loss: 39.0887\n",
      "Epoch 496/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 30.6620 - val_loss: 37.4219\n",
      "Epoch 497/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 32.9164 - val_loss: 45.6120\n",
      "Epoch 498/1000\n",
      "4091/4091 [==============================] - 1s 349us/sample - loss: 30.3265 - val_loss: 36.0173\n",
      "Epoch 499/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 28.9687 - val_loss: 38.6872\n",
      "Epoch 500/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 31.5720 - val_loss: 42.3162\n",
      "Epoch 501/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 30.4706 - val_loss: 45.0421\n",
      "Epoch 502/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 29.6833 - val_loss: 41.0023\n",
      "Epoch 503/1000\n",
      "4091/4091 [==============================] - 1s 318us/sample - loss: 30.4510 - val_loss: 40.3215\n",
      "Epoch 504/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 30.1321 - val_loss: 39.4027\n",
      "Epoch 505/1000\n",
      "4091/4091 [==============================] - 1s 345us/sample - loss: 31.1927 - val_loss: 37.2133\n",
      "Epoch 506/1000\n",
      "4091/4091 [==============================] - 1s 335us/sample - loss: 30.8815 - val_loss: 51.4362\n",
      "Epoch 507/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 30.2459 - val_loss: 40.0939\n",
      "Epoch 508/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 30.9453 - val_loss: 39.2492\n",
      "Epoch 509/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 33.0096 - val_loss: 39.3921\n",
      "Epoch 510/1000\n",
      "4091/4091 [==============================] - 1s 348us/sample - loss: 29.9463 - val_loss: 35.7389\n",
      "Epoch 511/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 32.2203 - val_loss: 38.7370\n",
      "Epoch 512/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 29.1611 - val_loss: 36.7356\n",
      "Epoch 513/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 29.4686 - val_loss: 41.0290\n",
      "Epoch 514/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 31.9174 - val_loss: 38.9578\n",
      "Epoch 515/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 29.3769 - val_loss: 42.9692\n",
      "Epoch 516/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 30.8024 - val_loss: 40.5716\n",
      "Epoch 517/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 29.6577 - val_loss: 36.2374\n",
      "Epoch 518/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 29.7340 - val_loss: 39.3932\n",
      "Epoch 519/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 29.2367 - val_loss: 36.9175\n",
      "Epoch 520/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 30.5368 - val_loss: 38.5969\n",
      "Epoch 521/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 29.4654 - val_loss: 36.5060\n",
      "Epoch 522/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 29.7055 - val_loss: 38.9059\n",
      "Epoch 523/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 29.7048 - val_loss: 38.6191\n",
      "Epoch 524/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 29.2718 - val_loss: 39.7137\n",
      "Epoch 525/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 29.3765 - val_loss: 41.8887\n",
      "Epoch 526/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 29.4304 - val_loss: 39.7477\n",
      "Epoch 527/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 30.1672 - val_loss: 39.9542\n",
      "Epoch 528/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 28.4530 - val_loss: 46.5055\n",
      "Epoch 529/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 28.7651 - val_loss: 36.4042\n",
      "Epoch 530/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 28.9423 - val_loss: 45.3741\n",
      "Epoch 531/1000\n",
      "4091/4091 [==============================] - 1s 337us/sample - loss: 31.5580 - val_loss: 37.8499\n",
      "Epoch 532/1000\n",
      "4091/4091 [==============================] - 1s 337us/sample - loss: 29.8248 - val_loss: 36.3765\n",
      "Epoch 533/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 28.6164 - val_loss: 44.2229\n",
      "Epoch 534/1000\n",
      "4091/4091 [==============================] - 1s 350us/sample - loss: 29.1361 - val_loss: 35.6831\n",
      "Epoch 535/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 28.7955 - val_loss: 36.2781\n",
      "Epoch 536/1000\n",
      "4091/4091 [==============================] - 1s 331us/sample - loss: 29.2857 - val_loss: 40.1002\n",
      "Epoch 537/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 29.2275 - val_loss: 46.3897\n",
      "Epoch 538/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 28.1921 - val_loss: 36.1594\n",
      "Epoch 539/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 29.0017 - val_loss: 36.8881\n",
      "Epoch 540/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 29.2388 - val_loss: 36.7954\n",
      "Epoch 541/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 28.4996 - val_loss: 37.4444\n",
      "Epoch 542/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 28.3854 - val_loss: 44.9196\n",
      "Epoch 543/1000\n",
      "4091/4091 [==============================] - 1s 334us/sample - loss: 28.5228 - val_loss: 37.2406\n",
      "Epoch 544/1000\n",
      "4091/4091 [==============================] - 1s 316us/sample - loss: 29.7936 - val_loss: 36.8840\n",
      "Epoch 545/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 28.5156 - val_loss: 41.5544\n",
      "Epoch 546/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 28.6995 - val_loss: 40.5261\n",
      "Epoch 547/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 30.3437 - val_loss: 40.8410\n",
      "Epoch 548/1000\n",
      "4091/4091 [==============================] - 1s 347us/sample - loss: 29.9092 - val_loss: 35.0458\n",
      "Epoch 549/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 28.7656 - val_loss: 36.3485\n",
      "Epoch 550/1000\n",
      "4091/4091 [==============================] - 1s 334us/sample - loss: 29.4272 - val_loss: 41.2072\n",
      "Epoch 551/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 28.3863 - val_loss: 45.7176\n",
      "Epoch 552/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 29.4767 - val_loss: 35.6885\n",
      "Epoch 553/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 29.1490 - val_loss: 40.6858\n",
      "Epoch 554/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 28.4631 - val_loss: 38.1414\n",
      "Epoch 555/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 29.2220 - val_loss: 40.7055\n",
      "Epoch 556/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 28.8955 - val_loss: 35.7703\n",
      "Epoch 557/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 29.0208 - val_loss: 41.1438\n",
      "Epoch 558/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 28.4652 - val_loss: 37.1008\n",
      "Epoch 559/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 27.1606 - val_loss: 43.4164\n",
      "Epoch 560/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 30.2393 - val_loss: 38.0588\n",
      "Epoch 561/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 29.5270 - val_loss: 37.7689\n",
      "Epoch 562/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 30.1142 - val_loss: 39.0099\n",
      "Epoch 563/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 28.5961 - val_loss: 35.8910\n",
      "Epoch 564/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 31.4106 - val_loss: 37.5720\n",
      "Epoch 565/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 28.2557 - val_loss: 35.3210\n",
      "Epoch 566/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 28.4302 - val_loss: 37.3179\n",
      "Epoch 567/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 29.7839 - val_loss: 39.0489\n",
      "Epoch 568/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 28.5341 - val_loss: 36.9838\n",
      "Epoch 569/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 30.3043 - val_loss: 37.2319\n",
      "Epoch 570/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 27.6259 - val_loss: 35.7268\n",
      "Epoch 571/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 28.6841 - val_loss: 38.3839\n",
      "Epoch 572/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 27.6205 - val_loss: 41.8191\n",
      "Epoch 573/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 28.9490 - val_loss: 35.9470\n",
      "Epoch 574/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 29.3088 - val_loss: 37.4357\n",
      "Epoch 575/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 28.3053 - val_loss: 41.0702\n",
      "Epoch 576/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 28.8149 - val_loss: 37.3834\n",
      "Epoch 577/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 28.5184 - val_loss: 41.4438\n",
      "Epoch 578/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 27.8267 - val_loss: 37.7195\n",
      "Epoch 579/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 29.4405 - val_loss: 45.8536\n",
      "Epoch 580/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 28.1970 - val_loss: 38.0226\n",
      "Epoch 581/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 27.7829 - val_loss: 37.1241\n",
      "Epoch 582/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 29.3434 - val_loss: 36.7805\n",
      "Epoch 583/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 27.3679 - val_loss: 38.3923\n",
      "Epoch 584/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 29.0892 - val_loss: 44.8534\n",
      "Epoch 585/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 27.9860 - val_loss: 38.5376\n",
      "Epoch 586/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 27.6614 - val_loss: 35.5076\n",
      "Epoch 587/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 27.3604 - val_loss: 36.8316\n",
      "Epoch 588/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 28.9065 - val_loss: 39.9336\n",
      "Epoch 589/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 28.7808 - val_loss: 41.8222\n",
      "Epoch 590/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 28.1287 - val_loss: 35.6193\n",
      "Epoch 591/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 27.6888 - val_loss: 37.1761\n",
      "Epoch 592/1000\n",
      "4091/4091 [==============================] - 1s 350us/sample - loss: 28.4037 - val_loss: 34.5807\n",
      "Epoch 593/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 27.3625 - val_loss: 37.5507\n",
      "Epoch 594/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 29.2734 - val_loss: 34.9186\n",
      "Epoch 595/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 27.2739 - val_loss: 40.2581\n",
      "Epoch 596/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 28.6540 - val_loss: 36.8473\n",
      "Epoch 597/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 28.9645 - val_loss: 41.8129\n",
      "Epoch 598/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 27.6534 - val_loss: 43.3457\n",
      "Epoch 599/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 27.1792 - val_loss: 39.3058\n",
      "Epoch 600/1000\n",
      "4091/4091 [==============================] - 1s 331us/sample - loss: 28.4643 - val_loss: 39.9392\n",
      "Epoch 601/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 27.7960 - val_loss: 39.2720\n",
      "Epoch 602/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 28.9714 - val_loss: 36.3013\n",
      "Epoch 603/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 26.9059 - val_loss: 35.1350\n",
      "Epoch 604/1000\n",
      "4091/4091 [==============================] - 1s 343us/sample - loss: 27.4104 - val_loss: 34.0631\n",
      "Epoch 605/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 27.6376 - val_loss: 43.8234\n",
      "Epoch 606/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 29.0134 - val_loss: 38.9634\n",
      "Epoch 607/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 26.8362 - val_loss: 39.7829\n",
      "Epoch 608/1000\n",
      "4091/4091 [==============================] - 1s 328us/sample - loss: 29.6974 - val_loss: 35.7201\n",
      "Epoch 609/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 26.9313 - val_loss: 37.3391\n",
      "Epoch 610/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 28.2349 - val_loss: 34.9191\n",
      "Epoch 611/1000\n",
      "4091/4091 [==============================] - 1s 315us/sample - loss: 26.7731 - val_loss: 34.6995\n",
      "Epoch 612/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 28.2775 - val_loss: 35.7252\n",
      "Epoch 613/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 28.2783 - val_loss: 36.4258\n",
      "Epoch 614/1000\n",
      "4091/4091 [==============================] - 1s 332us/sample - loss: 27.3070 - val_loss: 35.3324\n",
      "Epoch 615/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 25.7696 - val_loss: 34.8904\n",
      "Epoch 616/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 26.0516 - val_loss: 35.9422\n",
      "Epoch 617/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 28.3220 - val_loss: 44.5381\n",
      "Epoch 618/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 29.1420 - val_loss: 36.4404\n",
      "Epoch 619/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 26.1284 - val_loss: 41.3641\n",
      "Epoch 620/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 29.3696 - val_loss: 37.6601\n",
      "Epoch 621/1000\n",
      "4091/4091 [==============================] - 1s 327us/sample - loss: 27.0201 - val_loss: 35.8106\n",
      "Epoch 622/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 27.0130 - val_loss: 35.1838\n",
      "Epoch 623/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 26.2059 - val_loss: 35.1243\n",
      "Epoch 624/1000\n",
      "4091/4091 [==============================] - 1s 332us/sample - loss: 28.1275 - val_loss: 37.6290\n",
      "Epoch 625/1000\n",
      "4091/4091 [==============================] - 1s 342us/sample - loss: 27.3857 - val_loss: 36.9847\n",
      "Epoch 626/1000\n",
      "4091/4091 [==============================] - 1s 335us/sample - loss: 26.4693 - val_loss: 36.0578\n",
      "Epoch 627/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 28.4212 - val_loss: 41.1107\n",
      "Epoch 628/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 28.8487 - val_loss: 36.0921\n",
      "Epoch 629/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 27.0082 - val_loss: 40.9430\n",
      "Epoch 630/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 26.5313 - val_loss: 42.1998\n",
      "Epoch 631/1000\n",
      "4091/4091 [==============================] - 1s 322us/sample - loss: 26.6160 - val_loss: 34.1691\n",
      "Epoch 632/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 26.7999 - val_loss: 39.9979\n",
      "Epoch 633/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 28.1261 - val_loss: 34.5150\n",
      "Epoch 634/1000\n",
      "4091/4091 [==============================] - 1s 332us/sample - loss: 27.0073 - val_loss: 36.8633\n",
      "Epoch 635/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 26.8254 - val_loss: 39.6414\n",
      "Epoch 636/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 28.5850 - val_loss: 40.4180\n",
      "Epoch 637/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 28.7267 - val_loss: 34.5992\n",
      "Epoch 638/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 26.5491 - val_loss: 34.9543\n",
      "Epoch 639/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 28.2109 - val_loss: 38.3249\n",
      "Epoch 640/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 27.2702 - val_loss: 37.8801\n",
      "Epoch 641/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 27.7166 - val_loss: 38.5547\n",
      "Epoch 642/1000\n",
      "4091/4091 [==============================] - 1s 330us/sample - loss: 26.1477 - val_loss: 34.7820\n",
      "Epoch 643/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 27.6657 - val_loss: 40.9359\n",
      "Epoch 644/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 27.7499 - val_loss: 34.5188\n",
      "Epoch 645/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 25.3570 - val_loss: 35.4025\n",
      "Epoch 646/1000\n",
      "4091/4091 [==============================] - 1s 323us/sample - loss: 26.5869 - val_loss: 38.3698\n",
      "Epoch 647/1000\n",
      "4091/4091 [==============================] - 1s 321us/sample - loss: 28.5996 - val_loss: 38.4010\n",
      "Epoch 648/1000\n",
      "4091/4091 [==============================] - 1s 335us/sample - loss: 29.1373 - val_loss: 41.8002\n",
      "Epoch 649/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 25.8035 - val_loss: 37.3833\n",
      "Epoch 650/1000\n",
      "4091/4091 [==============================] - 1s 320us/sample - loss: 26.6594 - val_loss: 39.1334\n",
      "Epoch 651/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 27.3950 - val_loss: 37.5688\n",
      "Epoch 652/1000\n",
      "4091/4091 [==============================] - 1s 324us/sample - loss: 27.8108 - val_loss: 41.9537\n",
      "Epoch 653/1000\n",
      "4091/4091 [==============================] - 1s 326us/sample - loss: 26.4164 - val_loss: 35.2472\n",
      "Epoch 654/1000\n",
      "4091/4091 [==============================] - 1s 325us/sample - loss: 28.6295 - val_loss: 34.2408\n",
      "Epoch 00654: early stopping\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import concatenate, ZeroPadding2D, Add, add, Input, Conv2D, MaxPooling2D, BatchNormalization, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.initializers import Constant\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "dropout_rate=0\n",
    "resolution = 64\n",
    "channels = 1\n",
    "target=\"/special/jbpark/TabS6LData/Joonbeom/train_dataset/\"\n",
    "model_dir = \"/special/jbpark/EvalModel\"\n",
    "\n",
    "def custom_loss(y_true, y_pred): \n",
    "    #euclidean loss\n",
    "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1)) # /205.7\n",
    "    #squared loss\n",
    "    #return K.sum(K.square(y_pred - y_true), axis=-1) \n",
    "    \n",
    "SConv1 = Conv2D(32, (3, 3), padding = 'same', activation = 'relu')\n",
    "SConv2 = Conv2D(64, (3, 3), padding = 'same', activation = 'relu')\n",
    "SConv3 = Conv2D(64, (3, 3), padding = 'same', activation = 'relu')\n",
    "\n",
    "# Left Eye\n",
    "input1 = Input(shape=(64, 64,channels), name='left_eye')\n",
    "x = SConv1(input1)\n",
    "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "\n",
    "x = SConv2(x)\n",
    "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "\n",
    "x = SConv3(x)\n",
    "left_eye = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "left_eye = Flatten()(left_eye)\n",
    "\n",
    "# Right Eye\n",
    "input2 = Input(shape=(64, 64,channels), name='right_eye')\n",
    "x = SConv1(input2)\n",
    "x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "\n",
    "x = SConv2(x)\n",
    "x = MaxPooling2D(pool_size = (2, 2), padding='same')(x)\n",
    "\n",
    "x = SConv3(x)\n",
    "right_eye = MaxPooling2D(pool_size = (2, 2), padding='same')(x)\n",
    "right_eye = Flatten()(right_eye)\n",
    "\n",
    "# Eyes\n",
    "eyes = concatenate([left_eye, right_eye])\n",
    "fc1 = Dense(64, activation='relu')(eyes)\n",
    "fc2 = Dense(16, activation='relu')(fc1)\n",
    "fc2 = Dropout(rate=dropout_rate)(fc2)\n",
    "\n",
    "# Facepos\n",
    "input4 = Input(shape=(1, 1, 2), name='facepos')\n",
    "facepos = Flatten()(input4)\n",
    "\n",
    "#Euler\n",
    "input3 = Input(shape=(1, 1, 3), name='euler')\n",
    "euler = Flatten()(input3)\n",
    "\n",
    "# Eye size\n",
    "input5 = Input(shape=(1, 1, 2), name='left_eye_right_top')\n",
    "input6 = Input(shape=(1, 1, 2), name='right_eye_right_top')\n",
    "left_eye_right_top = Flatten()(input5)\n",
    "right_eye_right_top = Flatten()(input6)\n",
    "eye_corner = concatenate([left_eye_right_top, right_eye_right_top])\n",
    "\n",
    "head_pose = concatenate([euler, facepos, eye_corner])\n",
    "fc_f1 = Dense(16, activation='relu')(head_pose)\n",
    "\n",
    "# FC2, FC3\n",
    "fc2 = concatenate([fc2, fc_f1])\n",
    "fc2 = Dense(16, activation='relu')(fc2)\n",
    "fc3 = Dense(2, activation='linear', name='pred')(fc2)\n",
    "\n",
    "fc3 = add([fc3,facepos])\n",
    "pred = fc3\n",
    "\n",
    "model = Model(inputs=[input1, input2, input3, input4, input5, input6], outputs=[pred])\n",
    "\n",
    "tf.distribute.MirroredStrategy()\n",
    "\n",
    "model.compile(loss=custom_loss, optimizer=Adam(lr=1e-3))\n",
    "model.summary()\n",
    "\n",
    "gaze_point = np.load(target+\"gaze_point.npy\").astype(float)\n",
    "left_eye = np.load(target+\"left_eye.npy\").reshape(-1,resolution,resolution,channels)\n",
    "right_eye = np.load(target+\"right_eye.npy\").reshape(-1,resolution,resolution,channels)\n",
    "euler = np.load(target+\"euler.npy\").reshape(-1,1,1,3)\n",
    "facepos = np.load(target+\"facepos.npy\").reshape(-1,1,1,2)\n",
    "left_eye_right_top = np.load(target+\"left_eye_right_top.npy\")\n",
    "left_eye_left_bottom = np.load(target+\"left_eye_left_bottom.npy\")\n",
    "right_eye_right_top = np.load(target+\"right_eye_right_top.npy\")\n",
    "right_eye_left_bottom = np.load(target+\"right_eye_left_bottom.npy\")\n",
    "\n",
    "\n",
    "left_eye_right_top[:,1] = left_eye_right_top[:,1] - left_eye_left_bottom[:,1]\n",
    "left_eye_right_top[:,0] = left_eye_left_bottom[:,0] - left_eye_right_top[:,0]\n",
    "\n",
    "right_eye_right_top[:,1] = right_eye_right_top[:,1] - right_eye_left_bottom[:,1]\n",
    "right_eye_right_top[:,0] = right_eye_left_bottom[:,0] - right_eye_right_top[:,0]\n",
    "\n",
    "left_eye_right_top = left_eye_right_top.reshape(-1,1,1,2)\n",
    "right_eye_right_top = left_eye_left_bottom.reshape(-1,1,1,2)\n",
    "\n",
    "epoch = 1000\n",
    "Path(model_dir+'/checkpoint').mkdir(parents=True, exist_ok=True)\n",
    "mc = ModelCheckpoint(model_dir+'/checkpoint/gazel_shared_ver.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "#hist = model.fit([left_eye,right_eye, euler, facepos],gaze_point, validation_split=0.1,epochs=epoch, callbacks=[es, mc])\n",
    "hist = model.fit([left_eye, right_eye, euler, facepos, left_eye_right_top, right_eye_right_top,],gaze_point, validation_split=0.1,epochs=epoch, callbacks=[es, mc])\n",
    "\n",
    "Path(model_dir+'/Lmodels').mkdir(parents=True, exist_ok=True)\n",
    "model.save(model_dir+'/Lmodels/gazel_shared_ver.h5')\n",
    "\n",
    "K.clear_session()\n",
    "%xdel -n gaze_point\n",
    "%xdel -n left_eye\n",
    "%xdel -n right_eye\n",
    "%xdel -n euler\n",
    "%xdel -n facepos\n",
    "%xdel -n left_eye_right_top\n",
    "%xdel -n left_eye_left_bottom\n",
    "%xdel -n right_eye_right_top\n",
    "%xdel -n right_eye_left_bottom\n",
    "\n",
    "gc.collect()\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Lite Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ConverterError",
     "evalue": "See console for info.\n/bin/sh: 1: toco_from_protos: not found\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-68c152c5e8dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Convert the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Save the TF Lite model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tensorflow20/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0minput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0moutput_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         **converter_kwargs)\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_calibration_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tensorflow20/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[0;34m(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m       \u001b[0mdebug_info_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug_info_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m       enable_mlir_converter=enable_mlir_converter)\n\u001b[0m\u001b[1;32m    450\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/tensorflow20/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[0;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_convert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mConverterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"See console for info.\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# Must manually cleanup files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConverterError\u001b[0m: See console for info.\n/bin/sh: 1: toco_from_protos: not found\n\n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred): \n",
    "    #euclidean loss\n",
    "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1)) # /205.7\n",
    "    #squared loss\n",
    "    #return K.sum(K.square(y_pred - y_true), axis=-1) \n",
    "\n",
    "model_dir = \"/special/jbpark/EvalModel\"\n",
    "keras_file = model_dir+'/Lmodels/gazel_shared_ver.h5'\n",
    "# Convert to TensorFlow Lite model.\n",
    "model = load_model(keras_file,custom_objects={'custom_loss': custom_loss})\n",
    "\n",
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TF Lite model.\n",
    "Path(model_dir+'/tflite').mkdir(parents=True, exist_ok=True)\n",
    "with tf.io.gfile.GFile(model_dir+'/tflite/gazel_shared_ver.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### if you get error with toco_protos or toco run the above python code with terminal or convert whole file to .py file then run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### probably this error is due to my conda environment, if you use docker environment and latest tensorflow version 2.4.X you will not receive such error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "model_dir = \"/special/jbpark/EvalModel\"\n",
    "path = glob.glob(model_dir+'/tflite/gazel_shared_ver.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/special/jbpark/EvalModel/tflite/gazel_shared_ver.tflite\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'left_eye_right_top',\n",
       "  'index': 4,\n",
       "  'shape': array([1, 1, 1, 2], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0)},\n",
       " {'name': 'right_eye_right_top',\n",
       "  'index': 54,\n",
       "  'shape': array([1, 1, 1, 2], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0)},\n",
       " {'name': 'left_eye',\n",
       "  'index': 3,\n",
       "  'shape': array([ 1, 64, 64,  1], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0)},\n",
       " {'name': 'facepos',\n",
       "  'index': 2,\n",
       "  'shape': array([1, 1, 1, 2], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0)},\n",
       " {'name': 'right_eye',\n",
       "  'index': 53,\n",
       "  'shape': array([ 1, 64, 64,  1], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0)},\n",
       " {'name': 'euler',\n",
       "  'index': 1,\n",
       "  'shape': array([1, 1, 1, 3], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0)}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(path[0])\n",
    "tflite = tf.lite.Interpreter(model_path=path[0])\n",
    "tflite.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.0",
   "language": "python",
   "name": "tensorflow20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
